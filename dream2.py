# -*- coding: utf-8 -*-
"""Dream2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19ICLjXPIVI5F4SVVXreaiq4Lh67MVycy
"""

# Install required packages
!pip install pandas scikit-learn xgboost matplotlib seaborn pyod

# Import all libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc
from xgboost import XGBClassifier
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from pyod.models.knn import KNN
import joblib
import random

# Set random seed for reproducibility
np.random.seed(42)
random.seed(42)

# =============================================
# 1. Data Loading and Initial Exploration
# =============================================
print("STEP 1: DATA LOADING AND EXPLORATION")
print("====================================")

# Load the dataset
#from google.colab import files
#uploaded = files.upload()

# Read the CSV file
df = pd.read_csv('/content/ai4i2020.csv')
print(f"\nDataset shape: {df.shape}")
print("\nFirst 5 rows:")
print(df.head())

# Basic info
print("\nDataset info:")
print(df.info())

# =============================================
# 2. Enhanced Exploratory Data Analysis (EDA)
# =============================================
print("\nSTEP 2: ENHANCED EXPLORATORY DATA ANALYSIS")
print("=========================================")

# Missing values analysis
print("\nMissing values:")
print(df.isnull().sum())

# Target distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='Machine failure', data=df)
plt.title('Distribution of Machine Failures')
plt.show()

print("\nFailure distribution:")
print(df['Machine failure'].value_counts(normalize=True))

# Numerical features distribution
num_features = ['Air temperature [K]', 'Process temperature [K]',
                'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']

plt.figure(figsize=(15, 10))
for i, feature in enumerate(num_features, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df[feature], kde=True)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

# Correlation analysis
plt.figure(figsize=(10, 8))
corr_matrix = df[num_features].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix of Numerical Features')
plt.show()

# Pairplot of numerical features with failure indication
sns.pairplot(df, vars=num_features, hue='Machine failure',
             palette={0: 'blue', 1: 'red'}, plot_kws={'alpha': 0.5})
plt.suptitle('Pairplot of Numerical Features Colored by Failure Status', y=1.02)
plt.show()

# Tool wear vs failure
plt.figure(figsize=(10, 6))
sns.boxplot(x='Machine failure', y='Tool wear [min]', data=df)
plt.title('Tool Wear Distribution by Failure Status')
plt.show()

# =============================================
# 3. Feature Engineering
# =============================================
print("\nSTEP 3: FEATURE ENGINEERING")
print("===========================")

def create_features(df):
    # Create power feature (Torque * Rotational speed)
    df['Power'] = df['Torque [Nm]'] * df['Rotational speed [rpm]']

    # Create temperature difference
    df['Temp_diff'] = df['Process temperature [K]'] - df['Air temperature [K]']

    # Create torque to speed ratio
    df['Torque_speed_ratio'] = df['Torque [Nm]'] / (df['Rotational speed [rpm]'] + 0.001)

    # Create tool wear squared
    df['Tool_wear_squared'] = df['Tool wear [min]'] ** 2

    # Create overheating indicator
    df['Overheating'] = np.where(df['Process temperature [K]'] > 310, 1, 0)

    return df

df = create_features(df)

# Show new features
print("\nNew features created:")
print(df[['Power', 'Temp_diff', 'Torque_speed_ratio', 'Tool_wear_squared', 'Overheating']].head())

# =============================================
# 4. Data Preparation for Modeling
# =============================================
print("\nSTEP 4: DATA PREPARATION")
print("========================")

# Select features and target
features = [
    'Air temperature [K]',
    'Process temperature [K]',
    'Rotational speed [rpm]',
    'Torque [Nm]',
    'Tool wear [min]',
    'Power',
    'Temp_diff',
    'Torque_speed_ratio',
    'Tool_wear_squared',
    'Overheating'
]

target = 'Machine failure'

# Prepare data
X = df[features]
y = df[target]

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

print(f"\nTraining set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")
print(f"Class distribution in training set: {y_train.value_counts(normalize=True)}")

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# =============================================
# 5. Machine Failure Prediction Model
# =============================================
print("\nSTEP 5: MACHINE FAILURE PREDICTION MODEL")
print("=======================================")

# Train XGBoost model (good for imbalanced data)
model = XGBClassifier(
    random_state=42,
    scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train),  # Handle class imbalance
    eval_metric='logloss',
    n_estimators=200,
    max_depth=5,
    learning_rate=0.1
)

model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)[:, 1]  # Probability of failure

# Evaluate model
print("\nModel Evaluation:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# Feature importance
plt.figure(figsize=(12, 8))
sorted_idx = model.feature_importances_.argsort()
plt.barh(np.array(features)[sorted_idx], model.feature_importances_[sorted_idx])
plt.title('Feature Importance')
plt.show()

# =============================================
# 6. Anomaly Detection Implementation
# =============================================
print("\nSTEP 6: ANOMALY DETECTION")
print("========================")

# Initialize anomaly detection models
models = {
    "Isolation Forest": IsolationForest(n_estimators=150, contamination=0.05, random_state=42),
    "Local Outlier Factor": LocalOutlierFactor(n_neighbors=25, contamination=0.05),
    "K-Nearest Neighbors (KNN)": KNN(contamination=0.05)
}

# Fit models and detect anomalies
anomaly_results = {}
for name, model_ad in models.items():
    if name == "Local Outlier Factor":
        anomalies = model_ad.fit_predict(X_train_scaled)
    else:
        model_ad.fit(X_train_scaled)
        anomalies = model_ad.predict(X_train_scaled)

    # Convert predictions (1 = normal, -1 = anomaly)
    anomalies = np.where(anomalies == 1, 0, 1)
    anomaly_results[name] = anomalies

    # Count anomalies
    n_anomalies = sum(anomalies)
    print(f"{name} detected {n_anomalies} anomalies ({n_anomalies/len(X_train_scaled):.2%} of training data)")

# Add anomaly labels to dataframe
train_indices = X_train.index
for name in models.keys():
    col_name = name.replace(" ", "_") + "_Anomaly"
    df.loc[train_indices, col_name] = anomaly_results[name]

# Visualize anomalies
def plot_anomalies(feature1, feature2):
    plt.figure(figsize=(10, 6))
    sns.scatterplot(data=df.loc[train_indices], x=feature1, y=feature2,
                   hue='Isolation_Forest_Anomaly', palette={0: 'blue', 1: 'red'})
    plt.title(f'Anomaly Detection: {feature1} vs {feature2}')
    plt.show()

# Plot some example feature pairs
plot_anomalies('Rotational speed [rpm]', 'Torque [Nm]')
plot_anomalies('Process temperature [K]', 'Air temperature [K]')
plot_anomalies('Tool wear [min]', 'Power')

# =============================================
# 7. Risk Scoring System
# =============================================
print("\nSTEP 7: RISK SCORING SYSTEM")
print("==========================")

# Create a function to calculate risk scores
def calculate_risk_score(row):
    score = 0

    # Get failure probability
    scaled_data = scaler.transform([row[features]])
    failure_prob = model.predict_proba(scaled_data)[0, 1]
    score += failure_prob * 50  # Weighted contribution

    # Add points for each anomaly detection method
    for name in models.keys():
        col_name = name.replace(" ", "_") + "_Anomaly"
        if col_name in row and not pd.isna(row[col_name]):
            score += row[col_name] * (20 if "Isolation" in name else 15)

    # Additional risk factors
    if row['Tool wear [min]'] > 200:
        score += 10
    if row['Overheating'] == 1:
        score += 15

    # Cap at 100
    return min(100, score)

df['Risk_Score'] = df.apply(calculate_risk_score, axis=1)

# Visualize risk scores
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(data=df, x='Risk_Score', bins=20, kde=True)
plt.title('Distribution of Risk Scores')

plt.subplot(1, 2, 2)
sns.boxplot(x='Machine failure', y='Risk_Score', data=df)
plt.title('Risk Scores by Failure Status')
plt.tight_layout()
plt.show()

# Analyze high-risk cases
high_risk = df[df['Risk_Score'] > 70]
print(f"\nFound {len(high_risk)} high-risk cases (Risk Score > 70):")
print(high_risk[features + ['Machine failure', 'Risk_Score']].describe())

# =============================================
# 8. Model Testing with Random Sample
# =============================================
print("\nSTEP 8: TESTING WITH RANDOM SAMPLE")
print("=================================")

def evaluate_sample(sample, actual, X_train_scaled):
    """Evaluate a single sample with proper handling for anomaly detection"""
    try:
        # Prepare the sample
        sample_df = pd.DataFrame([sample])
        sample_df = create_features(sample_df)
        scaled_sample = scaler.transform(sample_df[features])

        # Get failure prediction
        failure_prob = model.predict_proba(scaled_sample)[0, 1]
        failure_pred = model.predict(scaled_sample)[0]

        # Initialize anomaly results
        anomaly_results = {}
        anomaly_scores = {}

        # Handle each anomaly detection model differently
        for name, model_ad in models.items():
            if name == "Local Outlier Factor":
                # For LOF, we need to include training data
                combined_data = np.vstack([X_train_scaled, scaled_sample])
                n_neighbors = min(25, len(X_train_scaled) - 1)
                lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=0.05)
                lof.fit(combined_data)
                anomaly_results[name] = lof.fit_predict(combined_data)[-1]  # Last element is our sample
                anomaly_scores[name] = lof.negative_outlier_factor_[-1]
            else:
                # Other models can handle single samples
                model_ad.fit(X_train_scaled)  # Refit on training data
                anomaly_results[name] = model_ad.predict(scaled_sample)[0]
                if hasattr(model_ad, 'decision_function'):
                    anomaly_scores[name] = model_ad.decision_function(scaled_sample)[0]
                else:
                    anomaly_scores[name] = np.nan

        # Calculate risk score
        risk_score = calculate_risk_score(sample_df.iloc[0])

        # Prepare results
        results = {
            'sample_index': random_idx,
            'actual_failure': bool(actual),
            'features': sample.to_dict(),
            'predictions': {
                'failure_probability': float(failure_prob),
                'predicted_failure': bool(failure_pred),
                'anomaly_detection': {
                    name: {
                        'result': 'Anomaly' if result == -1 else 'Normal',
                        'score': float(score)
                    } for name, (result, score) in zip(anomaly_results.keys(),
                                                     zip(anomaly_results.values(),
                                                         anomaly_scores.values()))
                }
            },
            'risk_assessment': {
                'risk_score': float(risk_score),
                'risk_level': 'High' if risk_score > 70 else ('Medium' if risk_score > 40 else 'Low')
            }
        }
        return results

    except Exception as e:
        print(f"Error evaluating sample: {str(e)}")
        return None

# Select and evaluate a random sample
random_idx = random.choice(X_test.index)
random_sample = X_test.loc[random_idx]
actual_failure = y_test.loc[random_idx]

# Evaluate with training data available for anomaly detection
sample_result = evaluate_sample(random_sample, actual_failure, X_train_scaled)

# Print results
if sample_result:
    print("\n" + "="*50)
    print(f"RANDOM SAMPLE EVALUATION (Index: {sample_result['sample_index']})")
    print("="*50)

    print(f"\n{'Actual Failure:':<25} {'Yes' if sample_result['actual_failure'] else 'No'}")
    print(f"{'Predicted Failure:':<25} {'Yes' if sample_result['predictions']['predicted_failure'] else 'No'}")
    print(f"{'Failure Probability:':<25} {sample_result['predictions']['failure_probability']:.4f}")

    print("\nANOMALY DETECTION RESULTS:")
    for algo, data in sample_result['predictions']['anomaly_detection'].items():
        print(f"{algo:<25} {data['result']} (Score: {data['score']:.2f})")

    print("\nRISK ASSESSMENT:")
    print(f"{'Risk Score:':<25} {sample_result['risk_assessment']['risk_score']:.1f}")
    print(f"{'Risk Level:':<25} {sample_result['risk_assessment']['risk_level']}")

    print("\nKEY FEATURE VALUES:")
    features_to_show = ['Tool wear [min]', 'Power', 'Process temperature [K]',
                       'Rotational speed [rpm]', 'Torque [Nm]']
    for feature in features_to_show:
        print(f"{feature:<25} {sample_result['features'][feature]:.2f}")

    print("\n" + "="*50)
else:
    print("Failed to evaluate sample.")

# =============================================
# 9. Model Saving
# =============================================
print("\nSTEP 9: SAVING MODELS")
print("=====================")

# Save models
joblib.dump(model, 'failure_prediction_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
for name, model_ad in models.items():
    filename = name.lower().replace(" ", "_") + "_anomaly.pkl"
    joblib.dump(model_ad, filename)

# Save the enhanced dataframe
df.to_csv('enhanced_predictive_maintenance.csv', index=False)

print("\nAll models and enhanced dataset saved successfully!")

!pip install -q -U google-generativeai flask_ngrok

!pip install -q -U google-generativeai fastapi nest-asyncio pyngrok uvicorn

# =============================================
# Predictive Maintenance Chat API with FastAPI
# =============================================
!pip install -q -U google-generativeai fastapi nest-asyncio pyngrok uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from pyngrok import ngrok
import nest_asyncio
import google.generativeai as genai
from pydantic import BaseModel
import json
import pandas as pd
import random
import threading

# Configure Gemini
GOOGLE_API_KEY = "AIzaSyBgwZWa8WkwfV1HPIR-P86GpkrCF4IEam4"  # Replace with your actual API key
genai.configure(api_key=GOOGLE_API_KEY)

# Initialize FastAPI app
app = FastAPI(title="Predictive Maintenance Chat API")

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =============================================
# Agent Definitions
# =============================================

class PredictiveAgent:
    def __init__(self):
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        self.name = "Predictive Analyst"
        self.instruction = """Analyze equipment sensor data to predict failures.
        Respond with:
        - failure_probability: 0-1
        - risk_factors: list
        - immediate_actions: list"""

    async def analyze(self, sensor_data: dict):
        prompt = f"""Analyze this industrial equipment data:
        {json.dumps(sensor_data, indent=2)}

        Provide:
        1. Probability of failure (0-1)
        2. Top 3 risk factors
        3. 2 recommended immediate actions"""

        response = await self.model.generate_content_async(prompt)
        return {
            "agent": self.name,
            "response": response.text,
            "type": "analysis"
        }

class GuidanceAgent:
    def __init__(self):
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        self.name = "Maintenance Advisor"
        self.instruction = """Create maintenance plans based on technical analysis.
        Respond with:
        - priority: Critical/High/Medium/Low
        - tasks: list
        - required_parts: list
        - estimated_downtime: hours"""

    async def advise(self, analysis: str):
        prompt = f"""Create maintenance plan for this analysis:
        {analysis}

        Include:
        1. Priority level
        2. Specific maintenance tasks
        3. Required parts/tools
        4. Estimated downtime in hours"""

        response = await self.model.generate_content_async(prompt)
        return {
            "agent": self.name,
            "response": response.text,
            "type": "guidance"
        }

class RemedyAgent:
    def __init__(self):
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        self.name = "Repair Specialist"
        self.instruction = """Provide step-by-step repair procedures.
        Respond with:
        - steps: numbered list
        - safety_checks: list
        - tools_required: list
        - time_per_step: minutes"""

    async def create_procedure(self, guidance: str):
        prompt = f"""Create repair procedure for:
        {guidance}

        For each step include:
        1. Tools/parts needed
        2. Detailed instructions
        3. Safety checks
        4. Time estimate in minutes"""

        response = await self.model.generate_content_async(prompt)
        return {
            "agent": self.name,
            "response": response.text,
            "type": "remedy"
        }

# Initialize agents
predictive_agent = PredictiveAgent()
guidance_agent = GuidanceAgent()
remedy_agent = RemedyAgent()

# =============================================
# Request/Response Models
# =============================================

class ChatRequest(BaseModel):
    message: str
    session_id: str = "default"

class ChatResponse(BaseModel):
    session_id: str
    agent: str
    response: str
    response_type: str

# =============================================
# API Endpoints
# =============================================

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    try:
        # Generate realistic sensor data
        sensor_data = {
            "air_temp_k": round(random.uniform(295, 310), 2),
            "process_temp_k": round(random.uniform(305, 320), 2),
            "rotational_speed_rpm": random.randint(1000, 3000),
            "torque_nm": round(random.uniform(30, 70), 2),
            "tool_wear_min": random.randint(0, 250)
        }

        user_input = request.message.lower()

        if "analyze" in user_input:
            response = await predictive_agent.analyze(sensor_data)
        elif "maintenance" in user_input or "plan" in user_input:
            analysis = await predictive_agent.analyze(sensor_data)
            response = await guidance_agent.advise(analysis["response"])
        elif "repair" in user_input or "procedure" in user_input:
            analysis = await predictive_agent.analyze(sensor_data)
            guidance = await guidance_agent.advise(analysis["response"])
            response = await remedy_agent.create_procedure(guidance["response"])
        else:
            response = {
                "agent": "System",
                "response": "Please specify what you need:\n- 'Analyze equipment'\n- 'Maintenance plan'\n- 'Repair procedure'",
                "type": "info"
            }

        return {
            "session_id": request.session_id,
            "agent": response["agent"],
            "response": response["response"],
            "response_type": response["type"]
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/sensor_data")
async def get_sensor_data():
    """Endpoint to get sample sensor data"""
    return {
        "air_temp_k": round(random.uniform(295, 310), 2),
        "process_temp_k": round(random.uniform(305, 320), 2),
        "rotational_speed_rpm": random.randint(1000, 3000),
        "torque_nm": round(random.uniform(30, 70), 2),
        "tool_wear_min": random.randint(0, 250)
    }

# =============================================
# Server Setup
# =============================================

def run_server():
    # Allow nested asyncio
    nest_asyncio.apply()

    # Start ngrok tunnel
    ngrok_tunnel = ngrok.connect(8000)
    print(f"Public URL: {ngrok_tunnel.public_url}")

    # Start FastAPI server
    uvicorn.run(app, host="0.0.0.0", port=8000)

# Start the server in a separate thread
threading.Thread(target=run_server, daemon=True).start()

print("""
=== Predictive Maintenance Chat API ===
Your FastAPI server is now running!

Available endpoints:
- POST /chat - Chat with the agents
- GET /sensor_data - Get sample sensor data

Try these commands:
1. "Analyze current equipment status"
2. "Suggest maintenance plan"
3. "Provide repair procedure"

The public URL will appear above when ngrok initializes.
""")

# =============================================
# 11 extra. First run this setup cell
# =============================================
!pip install -q -U google-generativeai fastapi nest-asyncio pyngrok uvicorn
!ngrok authtoken YOUR_NGROK_AUTH_TOKEN  # REPLACE WITH YOUR TOKEN

# =============================================
# 2. Then run this main cell
# =============================================
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import HTMLResponse
import uvicorn
from pyngrok import ngrok
import nest_asyncio
import google.generativeai as genai
import json
import random
import threading
from typing import Dict

# Configure Gemini
GOOGLE_API_KEY = "YOUR_API_KEY"  # REPLACE WITH YOUR KEY
genai.configure(api_key=GOOGLE_API_KEY)

# Initialize FastAPI
app = FastAPI()

# Agent Definitions
class PredictiveAgent:
    def __init__(self):
        self.model = genai.GenerativeModel('gemini-1.5-flash')
    async def analyze(self, sensor_data: Dict) -> Dict:
        response = await self.model.generate_content_async(
            f"Analyze this equipment data:\n{json.dumps(sensor_data, indent=2)}"
        )
        return {"agent": "Analyst", "response": response.text}

@app.post("/api/chat")
async def chat_endpoint(request: Dict):
    sensor_data = {
        "air_temp": round(random.uniform(295, 310), 2),
        "process_temp": round(random.uniform(305, 320), 2),
        "rpm": random.randint(1000, 3000),
        "torque": round(random.uniform(30, 70), 2),
        "tool_wear": random.randint(0, 250)
    }
    response = await PredictiveAgent().analyze(sensor_data)
    return response

@app.get("/", response_class=HTMLResponse)
async def chat_interface(request: Request):
    return """
    <html>
    <body>
        <h1>Predictive Maintenance Chat</h1>
        <div id="chat"></div>
        <input type="text" id="message">
        <button onclick="sendMessage()">Send</button>
        <script>
            async function sendMessage() {
                const response = await fetch('/api/chat', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({message: document.getElementById('message').value})
                });
                const data = await response.json();
                document.getElementById('chat').innerHTML +=
                    `<p><b>${data.agent}:</b> ${data.response}</p>`;
            }
        </script>
    </body>
    </html>
    """

# Start server with explicit URL printing
def run_server():
    nest_asyncio.apply()
    ngrok_tunnel = ngrok.connect(8000)
    print("\n=== Chat Interface Ready ===")
    print(f"Access at: {ngrok_tunnel.public_url}")
    print("\nIf the URL doesn't appear, check the ngrok dashboard:")
    print("https://dashboard.ngrok.com/status/tunnels")
    uvicorn.run(app, host="0.0.0.0", port=8000)

# Keep Colab alive
from IPython.display import Javascript
Javascript("""
function keepAlive() {
    console.log("Keeping colab alive");
    document.querySelector("colab-toolbar-button#connect").click()
}
setInterval(keepAlive, 60000)
""")

# Start server
threading.Thread(target=run_server, daemon=True).start()